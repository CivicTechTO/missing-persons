{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c78838",
   "metadata": {},
   "source": [
    "## Missing Persons DB Webscrape\n",
    "\n",
    "https://www.services.rcmp-grc.gc.ca/missing-disparus/search-recherche.jsf\n",
    "\n",
    "Search with no criteria will bring up all results.\n",
    "\n",
    "The links to the missing persons pages, can be appended to https://www.services.rcmp-grc.gc.ca/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f6db4d",
   "metadata": {},
   "source": [
    "### ========== Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512ae2b2",
   "metadata": {},
   "source": [
    "#### Get all the links to the more detailed case pages from the navigation pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf682cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554bc853",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://www.services.rcmp-grc.gc.ca/missing-disparus/search-recherche.jsf'\n",
    "# Get a chrome driver if there isn't one locally\n",
    "service = service=Service(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccac33e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(service=service) \n",
    "browser.get(link)\n",
    "time.sleep(2)\n",
    "\n",
    "# find search and click it to reach data\n",
    "try:\n",
    "    search = browser.find_element(By.NAME, 'searchForm:j_idt158')\n",
    "    search.click()\n",
    "except:\n",
    "    print('could not find Search')\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "not_last = True\n",
    "# to store all the URLs\n",
    "URLs = set()\n",
    "\n",
    "# go through each page and get all URLs    \n",
    "while not_last:\n",
    "    time.sleep(2) # wait more just in case\n",
    "    page = browser.page_source\n",
    "    pageSoup = bs(page, 'html.parser')\n",
    "    \n",
    "    # get all the links on the page and add them to array\n",
    "    pageURLs = pageSoup.find_all('a',class_=\"wet-newwindow\")\n",
    "    \n",
    "    print(\"Collecting URLs...\")\n",
    "    \n",
    "    # take each link on the page and add if not a dupe\n",
    "    for link in pageURLs:\n",
    "        href = link.get('href')\n",
    "        if href not in URLs:\n",
    "            URLs.add(\"https://www.services.rcmp-grc.gc.ca/\" + href)\n",
    "\n",
    "    print(\"Page Complete!\")\n",
    "    time.sleep(2) # wait a little\n",
    "    \n",
    "    # are we on the last page\n",
    "    try:\n",
    "        # click the next button at the bottom of the page\n",
    "        next_page = browser.find_element(By.XPATH, '/html/body/main/form/div[33]/ul/li[83]/a')\n",
    "        print('Found next button to press.')\n",
    "        next_page.click()\n",
    "        time.sleep(2) # wait for next page to load\n",
    "    except:\n",
    "        # should not have a next button on the last page\n",
    "        print('last page or no next button found!')\n",
    "        not_last = False\n",
    "    \n",
    "# the final list\n",
    "print(\"================================== END ==================================\")\n",
    "\n",
    "# write progress to csv\n",
    "df = pd.DataFrame(URLs, columns=[\"URL\"])\n",
    "df.to_csv('list.csv', index=False)\n",
    "\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef71b6a2",
   "metadata": {},
   "source": [
    "#### Collect all the data from all the detailed case pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77947f1a",
   "metadata": {},
   "source": [
    "This part of the code adapted from the CBC script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f6ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = r'https://www.services.rcmp-grc.gc.ca'\n",
    "\n",
    "#CLEANING FUNCTION\n",
    "def cleaning_function(item):\n",
    "    item = str(item)\n",
    "    item = item.replace(\"<dd>\" , \"\")\n",
    "    item = item.replace(\"</dd>\" , \"\")\n",
    "    item = item.replace(\"<p>\" , \"\")\n",
    "    item = item.replace(\"</p>\" , \"\")\n",
    "    item = item.replace(\"<strong>Missing from </strong>\" , \"\")\n",
    "    item = item.replace(\"<strong>\" , \"\")\n",
    "    item = item.replace(\"</strong>\" , \"\")\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d3b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the list where all the URLs from the sheet will go\n",
    "person_url_list = []\n",
    "#this is where all the person info will go\n",
    "person_info = []\n",
    "\n",
    "#a list for the sections later\n",
    "section_list = []\n",
    "\n",
    "#I have this because I dont know how else to filter out stuff from an if statement that I dont want\n",
    "count_working = 0\n",
    "\n",
    "for page_url in URLs:\n",
    "    print(\"Record Number: \" + str(count_working))\n",
    "    print(\"Case URL: \" + page_url)\n",
    "    count_working += 1\n",
    "    url = base_url + page_url\n",
    "    \n",
    "    # request the html\n",
    "    try:\n",
    "        page = requests.get(url, timeout = 10)\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"Timeout occurred\")\n",
    "    # structure the page content for parsing\n",
    "    soup = bs(page.content, 'html.parser') \n",
    "    \n",
    "    # First we have to pull out the content area\n",
    "    content_area = soup.find('main' , {\"property\" : \"mainContentOfPage\"})\n",
    "    \n",
    "    # LOCATION\n",
    "    try:\n",
    "        location_scrape = content_area.find('div')\n",
    "        location_isolate = location_scrape.find_all('p')\n",
    "        location_string = str(location_isolate[2])\n",
    "        location_split = location_string.split(\",\")\n",
    "        province = cleaning_function(location_split[1])\n",
    "        city = cleaning_function(location_split[0])\n",
    "    except:\n",
    "        print('No Location')\n",
    "    \n",
    "    # STATUS\n",
    "    status_scrape = content_area.find_all('h2')\n",
    "    status = status_scrape[:1]\n",
    "    status = str(status)\n",
    "    front_of_status = status.index('<h2>') + 4\n",
    "    back_of_status = status.index('</h2>')\n",
    "    status_cleaned = (f'{status[front_of_status : back_of_status]}')\n",
    "    \n",
    "    # FOR THE MISSING ENTRIES\n",
    "    if 'Missing' in status_cleaned:\n",
    "        #Now we get into pulling out individual details which will eventually be compiled in a list\n",
    "        #NAME(MISSING)\n",
    "        name_scrape = content_area.find_all('h3')\n",
    "        person_name = name_scrape[:1]\n",
    "        person_name = str(person_name)\n",
    "        front_of_name = person_name.index('<h3>') + 4\n",
    "        back_of_name = person_name.index('</h3>')\n",
    "        name_cleaned = (f'{person_name[front_of_name : back_of_name]}')\n",
    "        name_split = name_cleaned.split(',')\n",
    "        last_name = name_split[0]\n",
    "        first_name = name_split[1]\n",
    "        first_name_string = str(name_split[1:2])\n",
    "        first_name_string = first_name_string.replace('[',\"\")\n",
    "        first_name_string = first_name_string.replace(']',\"\")\n",
    "        first_name_string = first_name_string.replace(\"'\",\"\")\n",
    "        first_name_string = first_name_string.replace(\"\\n\",\"\")\n",
    "        first_name_string = first_name_string.strip()\n",
    "        \n",
    "        #PERSON DETAILS(MISSING)\n",
    "        try:\n",
    "            person_details = content_area.find_all('dd')\n",
    "            date_missing_discovered = person_details[0]\n",
    "            year_born = person_details[1]\n",
    "            age_at_disappearance = person_details[2]\n",
    "            gender = person_details[3]\n",
    "            bio_group = person_details[4]\n",
    "        except:\n",
    "            print('Data error')\n",
    "\n",
    "    #FOR THE UNIDENTIFIED ENTRIES\n",
    "    else:\n",
    "        try:\n",
    "            first_name_string = 'Unidentified'\n",
    "            last_name = 'Unidentified'\n",
    "            person_details = content_area.find_all('dd')\n",
    "            date_missing_discovered = person_details[0]\n",
    "            age_at_disappearance = person_details[1]\n",
    "            gender = person_details[2]\n",
    "            bio_group = person_details[3]\n",
    "            year_born = 'Unknown'\n",
    "        except:\n",
    "            print('Data error2')\n",
    "            \n",
    "            \n",
    "    #PUT IT ALL TOGETHER\n",
    "    person_info.append([first_name_string , last_name , status_cleaned , cleaning_function(date_missing_discovered) , cleaning_function(year_born) , cleaning_function(age_at_disappearance) , cleaning_function(gender) , cleaning_function(bio_group) , city , province , url])\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30062b69",
   "metadata": {},
   "source": [
    "### Save the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b297ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print to a file\n",
    "\n",
    "full_file = pd.DataFrame(person_info)\n",
    "full_file.to_csv(\"output_rcmp.csv\")\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e7fbf7",
   "metadata": {},
   "source": [
    "### To Avoid Running the URL Collector Again - Run Code Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5e73d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list.csv') as f:\n",
    "    allLines = f.readlines()\n",
    "    TempURLs = list(allLines)\n",
    "    # remove the column header\n",
    "    TempURLs = TempURLs[1:]\n",
    "    f.close()\n",
    "\n",
    "# clean the elements  \n",
    "URLs = []\n",
    "for link in TempURLs:\n",
    "    URLs.append(link.strip())\n",
    "    \n",
    "print(URLs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc0a32",
   "metadata": {},
   "source": [
    "#### Function to Turn DL sections into dict - No Longer Used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cad4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl_dict(soup):\n",
    "    keys, values = [] , []\n",
    "    for dl in soup.find_all(\"dl\", {\"class\":\"dl-horizontal\"}):\n",
    "        for dt in dl.find_all(\"dt\"):\n",
    "            keys.append(dt.text.strip())\n",
    "        for dd in dl.find_all(\"dd\"):\n",
    "            values.append(dd.text.strip())\n",
    "    \n",
    "    return dict(zip(keys,values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c611185",
   "metadata": {},
   "source": [
    "### Second Method - For More Structured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01a313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete file\n",
    "complete_db = {}\n",
    "\n",
    "# used to test\n",
    "test_URLs = URLs[:2]\n",
    "\n",
    "# loop through all the URLs\n",
    "for count, page_url in enumerate(URLs):\n",
    "    \n",
    "    # page dict\n",
    "    page_dict = {}\n",
    "    #this is where all the person info will go\n",
    "    page_sections = []\n",
    "    # make the full URL\n",
    "    url = page_url\n",
    "    \n",
    "    print('==============================================')\n",
    "    print(\"Record Number: \" + str(count))\n",
    "    print(\"Case URL: \" + url)\n",
    "        \n",
    "    # request the html\n",
    "    try:\n",
    "        page = requests.get(url, timeout = 10)\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"Timeout occurred\")\n",
    "    \n",
    "    # structure the page content for parsing\n",
    "    soup = bs(page.content, 'html.parser') \n",
    "    \n",
    "    #print(soup)\n",
    "    \n",
    "    # First we have to pull out the content area\n",
    "    content_area = soup.find('main' , {\"property\" : \"mainContentOfPage\"})\n",
    "    \n",
    "    try:\n",
    "        # the case reference number\n",
    "        _case_ref = content_area.find('h1')\n",
    "        page_dict['CaseRef'] = \" \".join(_case_ref.text.split())\n",
    "        \n",
    "        # the main section\n",
    "        sections = content_area.section\n",
    "        \n",
    "        # the description\n",
    "        desc = sections.div.p\n",
    "        page_dict['CaseDesc'] = desc.text.strip()\n",
    "        \n",
    "        # the category\n",
    "        case_type = sections.h2\n",
    "        page_dict['CaseType'] = \" \".join(case_type.text.split())\n",
    "    except:\n",
    "        print('page base info collection error')\n",
    "    \n",
    "    page_dict[\"CaseURL\"] = url\n",
    "    \n",
    "    \n",
    "    # find all the images in the persons section\n",
    "    try:\n",
    "        # the image link\n",
    "        images = sections.find_all('img')\n",
    "        imgs_list = []\n",
    "        for image in images:\n",
    "            image_src = image['src']\n",
    "            # check if this matches the no photo image\n",
    "            no_photo = re.search(\"noPhoto\\.png\", image_src)\n",
    "            if not no_photo:\n",
    "                # find the iamge ID\n",
    "                img_id = re.search(\"id=(\\d+).*\", image_src)\n",
    "                imgs_list.append(\"https://www.services.rcmp-grc.gc.ca/missing-disparus/showImage?\"+img_id.group())\n",
    "                # add the images section    \n",
    "        # add to the main dict\n",
    "        page_dict['PageImages'] = imgs_list\n",
    "    except:\n",
    "        print(\"no images found\")\n",
    "    \n",
    "    \"\"\"\n",
    "    # if we need to treat the page types differently\n",
    "    if page_dict['CaseType'] == 'Missing':\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the first section with all the persons\n",
    "    persons_section = sections.section\n",
    "    \n",
    "    # how many people are we looking through\n",
    "    persons_names = persons_section.find_all('h3')\n",
    "    num_persons = len(persons_names)\n",
    "    # all the blocks within the section\n",
    "    persons_blocks = persons_section.find_all('div',{\"class\":\"row\"})\n",
    "    \n",
    "    # loop through all the person sections to collect their data\n",
    "    # assigned to their names\n",
    "    for i in range(num_persons):\n",
    "        print(\"Person(s) in Case: \"+str(i+1))\n",
    "        block = {} # stores the individuals info, some pages have 1+\n",
    "        block['Name'] = \" \".join(persons_names[i].text.split())\n",
    "        \n",
    "        # select the current persion\n",
    "        current_person = persons_blocks[i]\n",
    "        \n",
    "        # takes all the DL sections out and saves them\n",
    "        dl_sections = []\n",
    "        for dl in current_person.find_all(\"dl\"):\n",
    "            dl_sections.append(str(dl))\n",
    "        block[\"InfoSection\"] = dl_sections          \n",
    "        # add the block to the page sections\n",
    "        page_sections.append(block)\n",
    "        print(block['Name'])\n",
    "        #print(persons_blocks[i])\n",
    "        #print(block)\n",
    "    \n",
    "    \"\"\"\n",
    "    # If this is an unidentified persons record\n",
    "    else:\n",
    "        print(\"Un IDs Body\")\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # write the section to the dict\n",
    "    page_dict['PersonsData'] = page_sections\n",
    "    # write it all to the main DB\n",
    "    complete_db[page_dict['CaseRef']] = page_dict\n",
    "        \n",
    "# write JSON to a file    \n",
    "with open(\"Complete_DB.json\", \"w\") as outfile:\n",
    "    json.dump(complete_db, outfile)\n",
    "        \n",
    "print('======================= Done =======================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc408dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in a string of <dl> ... </dl> and converts it into a dictionary\n",
    "def dl_to_dict(dl_str):\n",
    "    soup = bs(dl_str, 'html.parser')\n",
    "    #print(soup)\n",
    "\n",
    "    data = defaultdict(list)\n",
    "    dl = soup.find('dl')\n",
    "    k = ''\n",
    "    for c in sub.contents:\n",
    "        is_real = bool(str(c).strip())  # real element, not whitespace\n",
    "        if not is_real:\n",
    "            continue\n",
    "\n",
    "        if c.name == 'dt':\n",
    "            k = c.contents[0].strip()\n",
    "        elif c.name == 'dd':\n",
    "            data[k].append(c.contents[0].strip())\n",
    "            \n",
    "    return dict(data)\n",
    "            \n",
    "    \n",
    "data = dl_to_dict(complete_db[list(complete_db.keys())[0]]['PersonsData'][0]['InfoSection'][0])\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2516770f1231e9470eebfb2e8e89faf6b4b2c173f0d9550afe423a3c1e5f866c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
