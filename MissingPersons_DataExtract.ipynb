{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c78838",
   "metadata": {},
   "source": [
    "## Missing Persons DB Webscrape\n",
    "\n",
    "https://www.services.rcmp-grc.gc.ca/missing-disparus/search-recherche.jsf\n",
    "\n",
    "Search with no criteria will bring up all results.\n",
    "\n",
    "The links to the missing persons pages, can be appended to https://www.services.rcmp-grc.gc.ca/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f6db4d",
   "metadata": {},
   "source": [
    "### Code\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1219c0cb",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf682cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42525710",
   "metadata": {},
   "source": [
    "#### link to search results + install selenium if it is not already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554bc853",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://www.services.rcmp-grc.gc.ca/missing-disparus/search-recherche.jsf'\n",
    "# Get a chrome driver if there isn't one locally\n",
    "service = Service(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64da82c3",
   "metadata": {},
   "source": [
    "#### loop through all the search result pages and collect the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccac33e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(service=service) \n",
    "browser.get(link)\n",
    "time.sleep(2)\n",
    "\n",
    "# find search and click it to reach data\n",
    "try:\n",
    "    search = browser.find_element(By.NAME, 'searchForm:j_idt158')\n",
    "    search.click()\n",
    "except:\n",
    "    print('could not find Search')\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "# to store all the URLs\n",
    "URLs = set()\n",
    "\n",
    "# go through each page and get all URLs    \n",
    "while True:\n",
    "    time.sleep(2) # wait more just in case\n",
    "    page = browser.page_source\n",
    "    pageSoup = bs(page, 'html.parser')\n",
    "    \n",
    "    # get all the links on the page and add them to array\n",
    "    pageURLs = pageSoup.find_all('a',class_=\"wet-newwindow\")\n",
    "    \n",
    "    print(\"Collecting URLs...\")\n",
    "    \n",
    "    # take each link on the page and add if not a dupe\n",
    "    for link in pageURLs:\n",
    "        href = link.get('href')\n",
    "        if href not in URLs:\n",
    "            URLs.add(\"https://www.services.rcmp-grc.gc.ca\" + href)\n",
    "\n",
    "    print(\"Page Complete!\")\n",
    "    time.sleep(2) # wait a little\n",
    "    \n",
    "    # are we on the last page\n",
    "    try:\n",
    "        # click the next button at the bottom of the page\n",
    "        next_page = browser.find_element(By.XPATH, '/html/body/main/form/div[33]/ul/li[83]/a')\n",
    "        print('Found next button to press.')\n",
    "        next_page.click()\n",
    "        time.sleep(2) # wait for next page to load\n",
    "    except:\n",
    "        # should not have a next button on the last page\n",
    "        print('last page or no next button found!')\n",
    "        break\n",
    "\n",
    "# the final list\n",
    "print(\"================================== END ==================================\")\n",
    "\n",
    "# write progress to csv\n",
    "df = pd.DataFrame(URLs, columns=[\"URL\"])\n",
    "df.to_csv('list.csv', index=False)\n",
    "\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef71b6a2",
   "metadata": {},
   "source": [
    "## Collect all the data from all the detailed case pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3ac0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = r'https://www.services.rcmp-grc.gc.ca'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e7fbf7",
   "metadata": {},
   "source": [
    "#### To Avoid Running the URL Collector Again - Run Code Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5e73d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list.csv') as f:\n",
    "    allLines = f.readlines()\n",
    "    TempURLs = list(allLines)\n",
    "    # remove the column header\n",
    "    TempURLs = TempURLs[1:]\n",
    "\n",
    "# clean the elements  \n",
    "URLs = [link.strip() for link in TempURLs]\n",
    "    \n",
    "print(URLs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc0a32",
   "metadata": {},
   "source": [
    "#### Function to Turn DL sections into dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb583f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in a string of <dl> ... </dl> and converts it into a dictionary\n",
    "def dl_to_dict(dl_str):\n",
    "    soup = bs(dl_str, 'html.parser')\n",
    "    dl = soup.find('dl')\n",
    "    if not dl:\n",
    "        return {}\n",
    "    \n",
    "    data = defaultdict(list)\n",
    "    k = ''\n",
    "    for c in dl.contents:\n",
    "        is_real = bool(str(c).strip())  # real element, not whitespace\n",
    "        if not is_real:\n",
    "            continue\n",
    "\n",
    "        if c.name == 'dt':\n",
    "            k = c.contents[0].strip()\n",
    "        elif c.name == 'dd':\n",
    "            data[k].append(c.contents[0].strip())\n",
    "            \n",
    "    return dict(data)\n",
    "            \n",
    "# For Testing the Function\n",
    "\n",
    "#data = dl_to_dict(complete_db[list(complete_db.keys())[0]]['PersonsData'][0]['InfoSection'][0])\n",
    "#print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e84104",
   "metadata": {},
   "source": [
    "#### Looping through and scraping the data into a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01a313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_database(urls_list: list, iteration: int):\n",
    "\n",
    "    # complete file\n",
    "    complete_db = {}\n",
    "\n",
    "    # loop through all the URLs\n",
    "    for count, page_url in enumerate(urls_list):\n",
    "        \n",
    "        # page dict\n",
    "        page_dict = {}\n",
    "        #this is where all the person info will go\n",
    "        page_sections = []\n",
    "        # make the full URL\n",
    "        url = page_url\n",
    "        \n",
    "        print('==============================================')\n",
    "        print(\"Record Number: \" + str(count))\n",
    "        print(\"Case URL: \" + url)\n",
    "            \n",
    "        # request the html\n",
    "        try:\n",
    "            page = requests.get(url, timeout = 10)\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"Timeout occurred\")\n",
    "        \n",
    "        # structure the page content for parsing\n",
    "        soup = bs(page.content, 'html.parser')\n",
    "        \n",
    "        #print(soup)\n",
    "        \n",
    "        # First we have to pull out the content area\n",
    "        content_area = soup.find('main' , {\"property\" : \"mainContentOfPage\"})\n",
    "        \n",
    "        try:\n",
    "            # the case reference number\n",
    "            _case_ref = content_area.find('h1')\n",
    "            page_dict['CaseRef'] = \" \".join(_case_ref.text.split())\n",
    "            \n",
    "            # the main section\n",
    "            sections = content_area.section\n",
    "            \n",
    "            # the description\n",
    "            desc = sections.div.p\n",
    "            page_dict['CaseDesc'] = desc.text.strip()\n",
    "            \n",
    "            # the category\n",
    "            case_type = sections.h2\n",
    "            page_dict['CaseType'] = \" \".join(case_type.text.split())\n",
    "        except:\n",
    "            print('page base info collection error')\n",
    "        \n",
    "        page_dict[\"CaseURL\"] = url\n",
    "        \n",
    "        \n",
    "        # find all the images in the persons section\n",
    "        try:\n",
    "            # the image link\n",
    "            images = sections.find_all('img')\n",
    "            imgs_list = []\n",
    "            for image in images:\n",
    "                image_src = image['src']\n",
    "                # check if this matches the no photo image\n",
    "                no_photo = re.search(\"noPhoto\\.png\", image_src)\n",
    "                if not no_photo:\n",
    "                    # find the iamge ID\n",
    "                    img_id = re.search(\"id=(\\d+).*\", image_src)\n",
    "                    imgs_list.append(\"https://www.services.rcmp-grc.gc.ca/missing-disparus/showImage?\"+img_id.group())\n",
    "                    # add the images section    \n",
    "            # add to the main dict\n",
    "            page_dict['PageImages'] = imgs_list\n",
    "        except:\n",
    "            print(\"no images found\")\n",
    "        \n",
    "        \"\"\"\n",
    "        # if we need to treat the page types differently\n",
    "        if page_dict['CaseType'] == 'Missing':\n",
    "        \"\"\"\n",
    "        \n",
    "        # get the first section with all the persons\n",
    "        persons_section = sections.section\n",
    "        \n",
    "        # how many people are we looking through\n",
    "        persons_names = persons_section.find_all('h3')\n",
    "        num_persons = len(persons_names)\n",
    "        # all the blocks within the section\n",
    "        persons_blocks = persons_section.find_all('div',{\"class\":\"row\"})\n",
    "        \n",
    "        # loop through all the person sections to collect their data\n",
    "        # assigned to their names\n",
    "        for i in range(num_persons):\n",
    "            print(\"Person(s) in Case: \"+str(i+1))\n",
    "            block = {} # stores the individuals info, some pages have 1+\n",
    "            block['Name'] = \" \".join(persons_names[i].text.split())\n",
    "            \n",
    "            # select the current persion\n",
    "            current_person = persons_blocks[i]\n",
    "            \n",
    "            # array to save all the individual dl sections\n",
    "            dl_sections = []\n",
    "            \n",
    "            # takes all the DL sections out and saves them\n",
    "            for dl in current_person.find_all(\"dl\"):\n",
    "                # call the dl formatting function\n",
    "                dl_section_dict = dl_to_dict(str(dl))            \n",
    "                dl_sections.append(dl_section_dict)\n",
    "                \n",
    "            # append the formatted sectins array to the block\n",
    "            block[\"InfoSection\"] = dl_sections          \n",
    "            \n",
    "            # add the block to the page sections\n",
    "            page_sections.append(block)\n",
    "            print(block['Name'])\n",
    "        \n",
    "        # write the section to the dict\n",
    "        page_dict['PersonsData'] = page_sections\n",
    "        # write it all to the main DB\n",
    "        complete_db[page_dict['CaseRef']] = page_dict\n",
    "        \n",
    "    # fine name to write to\n",
    "    filename = \"data\\RCMP_Data_Part_\" + str(iteration) + \".json\"\n",
    "    \n",
    "    # write JSON to a file        \n",
    "    with open(filename, \"w\") as outfile:        \n",
    "        outfile.write(json.dumps(complete_db, indent = 2))\n",
    "            \n",
    "    print('======================= Done Part '+str(iteration)+' =======================')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc96268",
   "metadata": {},
   "source": [
    "#### Pageination of the data conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b78340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of cases in each subfile\n",
    "page_size = 100\n",
    "\n",
    "# for testing\n",
    "#test_urls = URLs[:250]\n",
    "#paginated_list = [test_urls[i:i+page_size] for i in range(0, len(test_urls), page_size)]\n",
    "\n",
    "# split the list into smaller parts\n",
    "paginated_list = [URLs[i:i+page_size] for i in range(0, len(URLs), page_size)]\n",
    "\n",
    "# loop through the divided list and output files\n",
    "for count, list_section in enumerate(paginated_list):\n",
    "    scrape_database(list_section, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149e2b21",
   "metadata": {},
   "source": [
    "#### Convert Page Sections into CSV Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117130d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the json files in the folder\n",
    "all_files = next(walk(\"data\\\\\"), (None, None, []))[2]\n",
    "json_files = []\n",
    "\n",
    "# get only the json files\n",
    "for file in all_files:\n",
    "    if file.endswith(\"json\"):\n",
    "        json_files.append(file)\n",
    "\n",
    "# create a df to store data\n",
    "main_df = pd.DataFrame(columns=[\"CaseRef\",\"CaseDesc\",\"CaseType\",\"CaseURL\",\"PageImages\",\"PersonsData\"])\n",
    "\n",
    "# loop through and convert to csv\n",
    "for file in json_files:\n",
    "    path = 'data/' + file\n",
    "    temp_df = pd.read_json(path)\n",
    "    temp_df_transposed = temp_df.transpose()    \n",
    "    main_df = pd.concat([main_df, temp_df_transposed])\n",
    "    \n",
    "main_df.to_csv(\"data\\json_converted.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2516770f1231e9470eebfb2e8e89faf6b4b2c173f0d9550afe423a3c1e5f866c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
